# Laboratorio-7
Este repositorio documenta la segunda tarea del curso, centrada en dos grandes componentes: por un lado un sistema de análisis de emociones faciales a partir de la cámara usando MediaPipe, hilos, exclusión mutua y semáforos con interfaz web en Streamlit y empaquetado en Docker, y por otro una exploración teórica de tecnologías de infraestructura y nube (Terraform, Ansible, RabbitMQ, OpenStack y el cuadro de Gartner); a nivel de entorno se asume Ubuntu (en el caso práctico 24.10), con los paquetes básicos instalados mediante `sudo apt update`, `sudo apt install -y software-properties-common`, activación del repositorio `universe` con `sudo add-apt-repository universe` y posterior `sudo apt update`, más la instalación de Python 3, `python3-venv`, `python3-pip`, git y docker con `sudo apt install -y python3 python3-venv python3-pip git docker.io`, comprobando el entorno con `python3 --version`, `pip3 --version` y `docker ps`. La estructura sugerida del repositorio es algo como: una carpeta `Sentimientos_Imagenes` con su `app.py`, `requirements.txt`, `Dockerfile` y archivos auxiliares, y una carpeta `docs/img` con las capturas y diagramas que se refieren más adelante, por ejemplo `sentimientos_streamlit.png`, `sentimientos_diagrama.png` y `tecnologias_mapa.png`, todo documentado en este README junto con un archivo adicional de texto para la parte teórica si se desea. Para aislar dependencias se puede usar un entorno virtual con `python3 -m venv .venv`, activándolo mediante `source .venv/bin/activate`, actualizando `pip` con `pip install --upgrade pip` y desactivándolo al terminar con `deactivate`, aunque en el proyecto de emociones la instalación real de dependencias ocurre dentro de la imagen de Docker (el entorno del host se usa principalmente para edición de código). En el componente de “Sentimientos por imágenes” el objetivo es detectar el estado “feliz”, “enojado” o “triste” a partir del rostro captado por la cámara, usando MediaPipe FaceMesh, dos hilos (uno para captura de video y otro para procesamiento), un `Lock` como mecanismo de exclusión mutua para las variables compartidas y un `Semaphore` para sincronizar la llegada de nuevos frames, presentando el resultado en tiempo real en una aplicación web hecha con Streamlit que se ejecuta dentro de un contenedor Docker basado en `python:3.11-slim` para garantizar compatibilidad con `mediapipe`. Dentro de `Sentimientos_Imagenes` se define un `requirements.txt` con `streamlit`, `opencv-python` y `mediapipe==0.10.11` y un `Dockerfile` que hace `FROM python:3.11-slim`, instala librerías de sistema necesarias para OpenCV (`ffmpeg libsm6 libxext6`), copia `requirements.txt`, ejecuta `pip install --no-cache-dir -r requirements.txt`, copia el resto de archivos a `/app`, expone el puerto 8501 y define como comando de inicio `streamlit run app.py --server.port=8501 --server.address=0.0.0.0`. El archivo `app.py` configura MediaPipe FaceMesh con un solo rostro y detección continua, declara variables globales compartidas (`frame_original`, `frame_procesado`, `sentimiento_actual`, un `Lock` llamado `lock_frame`, un semáforo `sem_nuevo_frame` y una bandera `running`), define una función `clasificar_sentimiento_por_landmarks` que toma los landmarks de boca, cejas, ojos, frente y barbilla, normaliza distancias respecto a la altura de la cara y calcula un “smile_score” (compara la altura del centro de la boca con la de las comisuras para distinguir entre sonrisa y gesto neutro/triste) y una medida de distancia ceja-ojo (cejas más bajas implican gesto de enfado), usando umbrales sencillos para devolver una de las tres etiquetas; otra función `procesar_frame` convierte el frame BGR a RGB, pasa por FaceMesh, dibuja los puntos clave, colorea una banda superior de la imagen en amarillo, rojo o azul según el sentimiento y escribe el texto “Sentimiento: …”; el hilo de captura abre `cv2.VideoCapture(0)`, en un bucle lee frames, entra en la sección crítica con `lock_frame` para actualizar `frame_original` y llama `sem_nuevo_frame.release()` para despertar al hilo de procesamiento, durmiendo unos milisegundos para mantener fluidez; el hilo de procesamiento espera con `sem_nuevo_frame.acquire()`, copia el último frame bajo el `Lock`, llama `procesar_frame`, y actualiza `frame_procesado` y `sentimiento_actual`. En la parte de Streamlit se configura la página con título y descripción, se usa `st.session_state["hilos_iniciados"]` para que los hilos se arranquen una sola vez (no cada recarga), se crean dos columnas, una con un `placeholder` de imagen y otra con un `placeholder` de texto, y se ejecuta un bucle que, si hay `frame_proce]()
![pruebaaaa](https://github.com/user-attachments/assets/2d27391d-d773-4b57-9ad1-7b1955fc3c7d)

